<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0078)http://courseweb.edteched.uottawa.ca/Medicine_Health/Concepts/Statistics_e.htm -->
<HTML><HEAD><TITLE>Significance vs. Importance</TITLE>
<META http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name="Author" content="Ian McDowell, University of Ottawa">
<meta name="Keywords" content="Medical education, Public health, Health, Social medicine, Social determinants">

<link href="../css/iphtitle.css" rel="stylesheet" type="text/css">
<link href="../css/iph.css" rel="stylesheet" type="text/css">
<link href="../css/iphsmall.css" rel="stylesheet" type="text/css">
</HEAD>
<BODY>
<img src="../images/banner_e.jpg" alt="English Banner" width="800" height="92">&nbsp; 
 
  <table width="897">
    <tr>
      <td width="446" class="iphtitle">Statistical Significance and Clinical Importance</td>
      <td width="127">&nbsp;</td>
      <td width="308" class="iph">To<a href="../epidemiology_biostatistics_e.html"> Epidemiology</a> theme page<br>
      To <a href="../evidence_based_medicine_e.html">EBM</a> theme page</td>
    </tr>
  </table>
  <P class="iphtitle">1. Core Knowledge:</P>
  <blockquote>
    <p class="iph">Significance is defined as the quality of being important. In medicine, we distinguish between statistical significance and clinical importance. </p>
    <p class="iph"><em><strong>Statistical Significance</strong></em>. Medical studies are  carried out on selected samples of people, but the goal is to apply the findings to another population (e.g., <em>your</em> patients).&nbsp; Naturally, a concern is that the sample used in the study could provide misleading results. Perhaps it was a very small sample; perhaps it was a biased sample that is not equivalent to the people you are treating; perhaps the sample was large enough, but by  chance or bad luck it contained people who gave wacky results. </p>
    <p class="iph">Statistical significance considers the first and third of these concerns. The middle one, bias, cannot be detected by  mathematical deductive logic: it needs detailed information on the way the sample was chosen. This is dealt with in the notes on <a href="EBM_Study_Bias_e.htm" target="_blank">bias</a>. </p>
    <p class="iph">Consider a study that shows a new therapy to be superior to the existing therapy. Statistical significance calculates the probability that the results observed in a study may have been merely a chance finding, and would not be repeated if the study were re-done. From the notes on the <a href="Statistics_logic_of_experimentation_e.htm" target="_blank">logic of experimentation</a>, you will recall that this depends on the sample size (the bigger the sample, the more confident you will be that it produces trustworthy  results) and the size of the difference observed. If the study showed a huge difference between new and old therapies,  the result is more likely to be real. &nbsp;<strong>Link</strong>: more on the <a href="Study_Design_Power_e.htm" target="_blank">statistical power</a> of a study </p>
    <p class="iph">Statistical significance in hypothesis testing is expressed in terms of a probability (hence that little letter &quot;<em>p</em>&quot;). By convention this is set at 5%, or <em>p</em> &lt; 0.05: there is only a 5% chance that a difference of the size found in your study, or a greater difference, would occur by chance, if there was actually no difference in the whole population. (In other words, you have drawn a false positive conclusion over the new therapy). The 5% value is arbitrary and is not chosen in terms of the actual magnitude of the effect seen in the study. Results are said to be &quot;statistically significant&quot; if the probability that the result is compatible with the null hypothesis is very small.</p>
    <blockquote>
      <p class="iph"> <em>Crucial Point:</em>  testing statistical significance  is all about the  likelihood of a chance finding that will not hold up in future replications.  Significance does not tell us directly <em>how big</em> the difference was. </p>
    </blockquote>
    <p class="iph"><em><strong>Clinical significance</strong>, or <strong>clinical importance</strong></em>: Is the difference between new and old therapy found in the study large enough for you to alter your practice? Because there is always a leap of faith in applying the results of a study to your patients (who, after all, were not in the study), perhaps  a small improvement in the new therapy is not sufficient to cause you to alter your clinical approach. Note that you would almost certainly <em>not</em> alter your approach if the study results were not statistically significant (i.e. could well have been due to chance). But when is the difference between two therapies large enough for you to alter your practice? </p>
    <p class="iph">Statistics cannot fully answer this question. It is one of clinical judgment, considering  the magnitude of benefit of each treatment, the respective profiles of side effects of the two treatments, their relative costs, your comfort with prescribing a new therapy, the patient's preferences, and so on. But we can provide different ways of illustrating the benefit of treatments, in terms of the <a href="Number_Needed_to_Treat_e.htm" target="_blank">Number Needed to Treat</a>. Yet another example of science offering only partial  guidance to the art of medicine. </p>
    <p class="iph">A partial way out of this uncertainty is to express study  results using <a href="Statistics_tests_e.htm#CI">confidence intervals</a> instead of significance levels. Confidence intervals show the likely range of results within which the true value is likely to lie. An example: a study showed a statistically significant impact (<em>p</em> &lt; 0.03)  of  Transcendental Meditation on reducing   systolic BP compared to controls. The mean reduction was  7 mm Hg  (95% CI 4, 10).  Instead of significance testing telling us that  this  study result could have occurred 3% of the time by chance alone, confidence intervals tell us what our best guess is for the size of the population effect, 95% of the time. This seems more informative for the clinician. </p>
    <p class="iph">An important idea to grasp is that if a study is very large, its  result may be statistically significant (= unlikely to be due to chance), and yet  the deviation from the null hypothesis may be too small to be of any clinical interest.&nbsp;  Conversely, the result may not be statistically significant because the study was so small (or &quot;under powered&quot;), but the difference is large and would seem potentially important from a clinical point of view.&nbsp; You will then be wise to do another, perhaps larger, study.</p>
    <p class="iph"><strong>Links</strong>: <br>
      Measures of the<a href="Statistics_impact_measures.htm"> impact of an exposure</a><br>
      The <a href="Study_Design_Power_e.htm">Statistical Power</a> of a study <br>
      Article on interpretation of statistical
      <a href="assets/documents/Statistical_Tests_P_Values.pdf">Significance Tests</a></p>
    <p class="iph">&quot;Statistics  &nbsp;are like bikinis. What they reveal is suggestive but what they conceal is  vital&quot; &nbsp;&nbsp;Aaron Levenstein</p>
<p class="iph">&nbsp;</p>
  </blockquote>

</BODY></HTML>